---
layout: page
---

<!-- > Nice to meet you !!!ğŸ˜Š Hope you are happy every day!!!ğŸ’¥ -->

## Biography

<img src="images/big_xinhao.jpg" style="float: right;" width="327" height="279">

> Nice to meet you !!!ğŸ˜Š

I am a MS student ğŸ™‚ at **Nanjing University** (2023.09â€”2026.06 if everything goes as expected), supervised by Prof. [*<font color="#006ab1">Limin Wang</font>*](https://scholar.google.com.hk/citations?user=HEuN8PcAAAAJ&hl=zh-CN&oi=ao). Previously, I received a bachelor degree from **Chongqing University** in 2023.06 (Major in computer science and technology ğŸ–¥ï¸, GPA: 3.9/4.0 overall rank: 1/295). I am also working as a research intern in [<font color="#006ab1">Shanghai AI lab</font>](https://www.shlab.org.cn/) (2023.07â€”present). Before that, I worked as an intern in [<font color="#006ab1">SenseTime</font>](https://www.sensetime.com) (2022.10â€”2023.06).

<!-- ## Academic Background

**<font color='red'>[Highlight]</font> I am looking for PhD to start in 2025 Fall. Contact me if you have any leads!** [talk with me](https://calendly.com/lancecai/meet-with-lance)

- **Sep 2020 - June 2024:** Fuzhou University (BEng)
- **Sep 2020 - May 2024:** Maynooth University (BSc)
- **June 2022 - Nov 2022:** Cambridge University (Intern)

<br>

--- -->

## Research Interests

My research interests lies at the computer vision and multimodal:

- Vision-Language Representation Learning
- Vision Foundation Models
- Multimodal Instruction-following Agents
- Parameter-Efficient Transfer Learning


I am particularly interested in the progress of the above direction in the field of **video understanding**. Currently, I am researching how to build and evaluate the long video mllms & video foundation models.



---

## News and Updates
- **Jan 2025:** ğŸ”¥ğŸ”¥ğŸ”¥We present [VideoChat-Flash](https://internvideo.github.io/blog/2024-12-31-VideoChat-Flash/) and [VideoChat-Online](https://videochat-online.github.io/), new video mllms and benchmarks for long video understanding.
- **Jul 2024:** Three papers accepted by ECCV2024.
- **Mar 2024:** ğŸ”¥ğŸ”¥ğŸ”¥We present [InternVideo2](https://arxiv.org/pdf/2403.15377.pdf), the currently largest (6B parameters) and most powerful video foundation model.
- **Mar 2024:** We present [VideoMamba](https://arxiv.org/abs/2403.06977), an efficient video backbone architecture with the potential to serve as an alternative to the video transformer architecture.
- **Jan 2024:** One paper accepted by ICLR2024 (spotlight).
- **Dec 2023:** Accepted by ğŸ¼.
- **June 2023** Happy to graduate from Chongqing University. Thank you to all my classmates and teachers.

<br>
