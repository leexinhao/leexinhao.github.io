---
layout: page
---

<!-- > Nice to meet you !!!😊 Feel free to contact me! Hope you are happy every day!!!💥 -->

## Biography

<img src="images/big_xinhao.jpg" style="float: right;" width="327" height="279">

> Nice to meet you !!!😊

I am a MS student 🙂 at **Nanjing University** (2023.09—2026.06 if everything goes as expected), supervised by Prof. [*<font color="#006ab1">Limin Wang</font>*](https://scholar.google.com.hk/citations?user=HEuN8PcAAAAJ&hl=zh-CN&oi=ao). Previously, I received a bachelor degree from **Chongqing University** in 2023.06 (Major in computer science and technology 🖥️, GPA: 3.9/4.0 overall rank: 1/295). I am also working as a research intern in [<font color="#006ab1">Shanghai AI lab</font>](https://www.shlab.org.cn/) (2023.07—present). Before that, I worked as an intern in [<font color="#006ab1">SenseTime</font>](https://www.sensetime.com) (2022.10—2023.06).

## Research Interests

My research interests lies at the computer vision and multimodal:

- Vision-Language Representation Learning
- Vision Foundation Models
- Multimodal Instruction-following Agents
- Parameter-Efficient Transfer Learning


I am particularly interested in the progress of the above direction in the field of **video understanding**. Currently, I am researching how to build and evaluate the long video mllms & video reasoning models.



---

## News and Updates

- **Feb 2025:** One paper accepted by ICLR2025 and two papers accepted by CVPR2025.

- **Jan 2025:** 🔥🔥🔥We present [VideoChat-Flash](https://internvideo.github.io/blog/2024-12-31-VideoChat-Flash/) and [VideoChat-Online](https://videochat-online.github.io/), new video mllms and benchmarks for long video understanding.
- **Jul 2024:** Three papers accepted by ECCV2024.
- **Mar 2024:** 🔥🔥🔥We present [InternVideo2](https://arxiv.org/pdf/2403.15377.pdf), the currently largest (6B parameters) and most powerful video foundation model.
- **Mar 2024:** We present [VideoMamba](https://arxiv.org/abs/2403.06977), an efficient video backbone architecture with the potential to serve as an alternative to the video transformer architecture.
- **Jan 2024:** One paper accepted by ICLR2024 (spotlight).
- **Dec 2023:** Accepted by 🐼.
- **June 2023** Happy to graduate from Chongqing University. Thank you to all my classmates and teachers.

<br>
