---
layout: page
permalink: /publications/index.html
title: Publications
---


# Selected
\* equal contribution

> ***InternVideo2: Scaling Foundation Models for Multimodal Video Understanding.*** [[Paper]](https://arxiv.org/pdf/2403.15377.pdf)[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)

Yi Wang\*, Kunchang Li\*, **Xinhao Li\***, Jiashuo Yu\*, Yinan He\*, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, SongZe Li, hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang (\* Equal contribution) **(ECCV2024)**

> ***VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling*** [[Paper]](https://arxiv.org/abs/2501.00574)[[Code]](https://github.com/OpenGVLab/VideoChat-Flash)

**Xinhao Li\***, Yi Wang\*, Jiashuo Yu\*, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang

> ***Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method*** [[Paper]](https://arxiv.org/abs/2501.00584v1)

Zhenpeng Huang\*, **Xinhao Li\***, Jiaqi Li\*, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, Limin Wang  **(CVPR2025)**

> ***VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model.*** [[Paper]](https://arxiv.org/abs/2407.06491)[[Code]](https://github.com/leexinhao/VideoEval)

**Xinhao Li**, Zhenpeng Huang, Jing Wang, Kunchang Li, Limin Wang

> ***ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video.*** [[Paper]](https://arxiv.org/abs/2310.01324)[[Code]](https://github.com/leexinhao/ZeroI2V)

**Xinhao Li**, Yuhan Zhu, Limin Wang **(ECCV2024)**

# All

## 2025

> ***VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling*** [[Paper]](https://arxiv.org/abs/2501.00574)[[Code]](https://github.com/OpenGVLab/VideoChat-Flash)

**Xinhao Li\***, Yi Wang\*, Jiashuo Yu\*, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, Limin Wang


> ***Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method*** [[Paper]](https://arxiv.org/abs/2501.00584v1)

Zhenpeng Huang\*, **Xinhao Li\***, Jiaqi Li\* Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, Limin Wang  **(CVPR2025)**


> ***Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment.*** [[Paper]](https://arxiv.org/abs/2412.19326)[[Code]](https://github.com/OpenGVLab/TPO)

Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, **Xinhao Li**, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, Limin Wang, Yi Wang  **(CVPR2025)**

> ***Fine-grained Video-Text Retrieval: A New Benchmark and Method*** [[Paper]](https://arxiv.org/pdf/2501.00513)

Yifan Xu, **Xinhao Li**, Yichun Yang, Rui Huang, Limin Wang
## 2024

> ***VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model.*** [[Paper]](https://arxiv.org/abs/2407.06491)[[Code]](https://github.com/leexinhao/VideoEval)

**Xinhao Li**, Zhenpeng Huang, Jing Wang, Kunchang Li, Limin Wang

> ***VideoMamba: State Space Model for Efficient Video Understanding.*** [[Paper]](https://arxiv.org/abs/2403.06977)[[Code]](https://github.com/OpenGVLab/VideoMamba)

Kunchang Li, **Xinhao Li**, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao **(ECCV2024)**

> ***InternVideo2: Scaling Foundation Models for Multimodal Video Understanding.*** [[Paper]](https://arxiv.org/pdf/2403.15377.pdf)[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)

Yi Wang\*, Kunchang Li\*, **Xinhao Li\***, Jiashuo Yu\*, Yinan He\*, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, SongZe Li, hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang (\* Equal contribution) **(ECCV2024)**

> ***TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning.*** [[Paper]](https://arxiv.org/abs/2410.19702)

Xiangyu Zeng, Kunchang Li, Chenting Wang, **Xinhao Li**, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, Yali Wang, Yu Qiao, Limin Wang **(ICLR2025)**

## 2023

> ***ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video.*** [[Paper]](https://arxiv.org/abs/2310.01324)[[Code]](https://github.com/leexinhao/ZeroI2V)

**Xinhao Li**, Yuhan Zhu, Limin Wang **(ECCV2024)**

> ***InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation.*** [[Paper]](https://openreview.net/forum?id=MLBdiWu4Fw&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions))[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) 

Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, **Xinhao Li**, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao **(ICLR2024 spotlight)** 
