---
layout: page
permalink: /publications/index.html
title: Publications
---

## 2024

> ***VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model.*** [[Paper]](https://arxiv.org/abs/2407.06491)[[Code]](https://github.com/leexinhao/VideoEval)

**Xinhao Li**, Zhenpeng Huang, Jing Wang, Kunchang Li, Limin Wang

> ***VideoMamba: State Space Model for Efficient Video Understanding.*** [[Paper]](https://arxiv.org/abs/2403.06977)[[Code]](https://github.com/OpenGVLab/VideoMamba)

Kunchang Li, **Xinhao Li**, Yi Wang, Yinan He, Yali Wang, Limin Wang, Yu Qiao **(ECCV2024)**

> ***InternVideo2: Scaling Foundation Models for Multimodal Video Understanding.*** [[Paper]](https://arxiv.org/pdf/2403.15377.pdf)[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2)

Yi Wang\*, Kunchang Li\*, **Xinhao Li\***, Jiashuo Yu\*, Yinan He\*, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, SongZe Li, hongjie Zhang, Yifei Huang, Yu Qiao, Yali Wang, Limin Wang (\* Equal contribution) **(ECCV2024)**

## 2023

> ***ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video.*** [[Paper]](https://arxiv.org/abs/2310.01324)[[Code]](https://github.com/leexinhao/ZeroI2V)

**Xinhao Li**, Yuhan Zhu, Limin Wang **(ECCV2024)**

> ***InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation.*** [[Paper]](https://openreview.net/forum?id=MLBdiWu4Fw&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions))[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) 

Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, **Xinhao Li**, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao **(ICLR2024 spotlight)** 
